{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "activation_functions.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Machine-Learning-Tokyo/DL-workshop-series/blob/master/Part%20II%20-%20Learning%20in%20Deep%20Networks/activation_functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "-hyHvpe83O33",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Activation functions:\n",
        "- Tanh\n",
        "- Sigmoid\n",
        "- Softmax\n",
        "- ReLU\n",
        "- Leaky ReLU\n",
        "- Custom activation (e.g. x<sup>2</sup>)"
      ]
    },
    {
      "metadata": {
        "id": "gZWP-TdtNfkw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Why we need activation functions?\n",
        "The operations that take place in the Fully Connected and Convolution layers are linear functions.\n",
        "Having two or more layers of linear functions stack over is pointless since the result can be obtained by using only one linear layer:\n",
        "\n",
        "---\n",
        "f<sub>1</sub>(x) = a<sub>1</sub>x + b<sub>1</sub>\n",
        "\n",
        "f<sub>2</sub>(x) = a<sub>2</sub>x + b<sub>2</sub>\n",
        "\n",
        "\n",
        "---\n",
        "f<sub>1</sub>(f<sub>2</sub>(x)) = a<sub>1</sub>(a<sub>2</sub>x+b<sub>2</sub>) + b<sub>1</sub>\n",
        "\n",
        "f<sub>1</sub>(f<sub>2</sub>(x)) = (a<sub>1</sub>a<sub>2</sub>)x + (a<sub>1</sub>b<sub>2</sub> + b<sub>1</sub>)\n",
        "\n",
        "---\n",
        "\n",
        "f<sub>1</sub>(f<sub>2</sub>(x)) = a<sub>3</sub>x + b<sub>3</sub>\n",
        "\n",
        "a<sub>3</sub> = a<sub>1</sub>a<sub>2</sub>,\n",
        "\n",
        "b<sub>3</sub> = a<sub>1</sub>b<sub>2</sub> + b<sub>1</sub>\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "21wxfFikan75",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "In order to take advantage of multiple layers we need to add some non linearity in the system:\n",
        "\n",
        "- tanh:\n",
        "$$tanh(x) = \\frac{e^z-e^{-z}}{e^z+e^{-z}}$$\n",
        "\n",
        "- sigmoid:\n",
        "\n",
        "$$\\sigma(x) = \\frac{1}{1+e^{-x}}$$\n",
        "\n",
        "- Rectified Linear Unit (ReLU):\n",
        "$$r(x) = max(0, x)$$\n",
        "\n",
        "- Leaky Rectified Linear Unit (LeakyReLU):\n",
        "$$ lr(x) = \\begin{cases} ax &\\text{if x < 0}\\\\x&\\text{if x $\\geq$ 0} \\end{cases}$$\n",
        "\n",
        "- softmax:\n",
        "$$s(x_i) = \\frac{e^{x_i}}{\\sum_je^{x_j}}$$\n"
      ]
    },
    {
      "metadata": {
        "id": "yR7vJ4oC3G2I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.layers import Activation, LeakyReLU\n",
        "from keras.models import Sequential\n",
        "import keras.backend as K\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S3M1GMJcQlA_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "First we define a set of points on the y = x line"
      ]
    },
    {
      "metadata": {
        "id": "dtBry15R3fmq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x = np.arange(-3, 3.5, 0.5)\n",
        "plt.plot(x, x, '-')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EM1FUxpxQySB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Then we build a keras model with one layer: the activation layer.\n",
        "\n",
        "In our example we use *tanh()* as activation function.\n",
        "\n",
        "We pass the numbers through the model and get the outputs.\n",
        "\n",
        "Then we plot both the original points and the model outputs together to see the transformation"
      ]
    },
    {
      "metadata": {
        "id": "r6mEGWfnQUvi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "K.clear_session()\n",
        "model = Sequential([Activation('tanh', input_shape=(1,))])\n",
        "y_act = model.predict(x)\n",
        "\n",
        "plt.plot(x, x, '-')\n",
        "plt.plot(x, y_act, '--')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tsG9rPv7Rhcs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now let's do it for different activation functions and plot the results"
      ]
    },
    {
      "metadata": {
        "id": "8K__iRqF4lza",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "activations = ['tanh', 'sigmoid', 'relu']\n",
        "y_acts = []\n",
        "for activation in activations:\n",
        "  K.clear_session()\n",
        "  model = Sequential([Activation(activation, input_shape=(1,))])\n",
        "  y_act = model.predict(x)\n",
        "  y_acts.append(y_act)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nKJZ0XxDR6Lu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Since leaky relu cannot be passed as string argument, we treat it separately"
      ]
    },
    {
      "metadata": {
        "id": "CkTaYJ_m_7ob",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "K.clear_session()\n",
        "model = Sequential([LeakyReLU(0.5, input_shape=(1,))])\n",
        "y_act = model.predict(x)\n",
        "y_acts.append(y_act)\n",
        "activations.append('leakyRelu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "csoyrTi9SFJL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In the case of softmax if we pass each number by itself it will always output 1.\n",
        "\n",
        "Thus we pass all the numbers together and get the corresponding probabilities (each output is in [0, 1] and the sum is 1)"
      ]
    },
    {
      "metadata": {
        "id": "NFzX5HeUAaGg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "K.clear_session()\n",
        "model = Sequential([Activation('softmax', input_shape=(len(x),))])\n",
        "x_sm = np.expand_dims(x, 0)\n",
        "y_act = model.predict(x_sm)\n",
        "y_acts.append(y_act[0])\n",
        "activations.append('softmax')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pQ6e-QjM5lUv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.plot(x, x, 'o', label='original data')\n",
        "\n",
        "for y_act, activation in zip(y_acts, activations):\n",
        "  ax.plot(x, y_act, '--', label=activation)\n",
        "\n",
        "legend = ax.legend(loc='lower right', fontsize='large')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C_brCcNJJaZ5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can also define a custom activation function using the ordinary functions and keras backend"
      ]
    },
    {
      "metadata": {
        "id": "bwB5olWeDb7G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sq_activation(x):\n",
        "  return x**2\n",
        "\n",
        "K.clear_session()\n",
        "model = Sequential([Activation(sq_activation, input_shape=(1,))])\n",
        "y_act = model.predict(x)\n",
        "activations.append('sq_activation')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HSITewUfEdnG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.plot(x, x, '-', label='original data')\n",
        "ax.plot(x, x**2, 'o', label='x^2')\n",
        "ax.plot(x, y_act, '--', label='activation')\n",
        "\n",
        "legend = ax.legend(loc='lower right', fontsize='large')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}